{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://github.com/callysto/callysto-sample-notebooks/blob/master/notebooks/images/Callysto_Notebook-Banner_Top_06.06.18.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book - Alice's Adventures in Wonderland\n",
    "\n",
    "**Submitted by: A, B, C, D**\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"data/John_Tenniel-_Alice's_mad_tea_party,_colour.jpg\" alt=\"Alice's Tea Party\" style=\"width: 432px;\"/> </td>\n",
    "<td> <img src=\"data/Alice_White_Rabbit.jpeg\" alt=\"Alice Meeting White Rabbit\" style=\"width: 240px;\"/> </td>\n",
    "</tr></table>\n",
    "    \n",
    "[Alice's Adventures in Wonderland](https://en.wikipedia.org/wiki/Alice's_Adventures_in_Wonderland) is one of the most popular fiction novels among adults as well as children. It was written in 1865 by English author Charles Lutwidge Dodgson. \n",
    "\n",
    "On a regular day you might be reading the book and speculating about what will happen next. However in this hackathon you will encounter some interesting findings about the book while learning some new coding/hacking skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting ready\n",
    "\n",
    "This section sets up many things behind the scenes which are required for the rest of this notebook. Most of the code blocks in this section are ready-to-run so you won't have to do any modifications. You don't need to know everything about various tasks being accomplished by the code cell in this section to complete the challenges. However feel free to ask mentors about anything that makes you curious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install/Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to download and install required Python libraries. It may take couple of minutes to complete the execution of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spaCy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/d8/26120ca4a5b1a6fe215ccb01185cee41098eb61386b3fd3a7b28c085146a/spacy-2.2.3-cp37-cp37m-macosx_10_6_intel.whl (14.2MB)\n",
      "\u001b[K     |████████████████████████████████| 14.2MB 1.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting srsly<1.1.0,>=0.1.0 (from spaCy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/ab/fa09de5430537c12f445d049f3a2bad75cee160e81aae3455d34977c1e0e/srsly-0.2.0-cp37-cp37m-macosx_10_6_intel.whl (275kB)\n",
      "\u001b[K     |████████████████████████████████| 276kB 2.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0 (from spaCy)\n",
      "  Downloading https://files.pythonhosted.org/packages/b9/bd/faace403086ee922afc74e5615cb8c21020fcf5d5667314e943c08f71fde/murmurhash-1.0.2-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /anaconda3/lib/python3.7/site-packages (from spaCy) (1.16.4)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spaCy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/34/a8b682ee9b57db35a5fe4e179b77c1bda0f6a09745669a99cfc27aa2bed7/cymem-2.0.3-cp37-cp37m-macosx_10_6_intel.whl (54kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 22.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2 (from spaCy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/3e/9aaba1f8c0cb69e57ebeb411cc1b65b3f6bfc3572dd68969a6d3e59288f6/preshed-3.0.2-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (211kB)\n",
      "\u001b[K     |████████████████████████████████| 215kB 2.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting plac<1.2.0,>=0.9.6 (from spaCy)\n",
      "  Downloading https://files.pythonhosted.org/packages/86/85/40b8f66c2dd8f4fd9f09d59b22720cffecf1331e788b8a0cab5bafb353d1/plac-1.1.3-py2.py3-none-any.whl\n",
      "Collecting blis<0.5.0,>=0.4.0 (from spaCy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d8/f0be9d8ebec9cbeea1427de6ac0ecc919c0bfe881eff2d2965dbc310ca8b/blis-0.4.1-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (4.0MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0MB 2.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wasabi<1.1.0,>=0.4.0 (from spaCy)\n",
      "  Downloading https://files.pythonhosted.org/packages/3f/35/6dc35bc3b49e160a016e420eb4bdcf1c887db6fd33a463959c06a508c339/wasabi-0.4.0-py3-none-any.whl\n",
      "Collecting catalogue<1.1.0,>=0.0.7 (from spaCy)\n",
      "  Downloading https://files.pythonhosted.org/packages/4f/d5/46ff975f0d7d055cf95557b944fd5d29d9dfb37a4341038e070f212b24fe/catalogue-0.0.8-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /anaconda3/lib/python3.7/site-packages (from spaCy) (41.0.1)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /anaconda3/lib/python3.7/site-packages (from spaCy) (2.22.0)\n",
      "Collecting thinc<7.4.0,>=7.3.0 (from spaCy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/87/c7da01c45bf3c138242f19f09b987d0ffa632ac7e8508527ee34d3ebbd81/thinc-7.3.1-cp37-cp37m-macosx_10_6_intel.whl (3.0MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0MB 707kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting importlib-metadata>=0.20; python_version < \"3.8\" (from catalogue<1.1.0,>=0.0.7->spaCy)\n",
      "  Downloading https://files.pythonhosted.org/packages/f6/d2/40b3fa882147719744e6aa50ac39cf7a22a913cbcba86a0371176c425a3b/importlib_metadata-0.23-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spaCy) (2019.6.16)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spaCy) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spaCy) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spaCy) (1.24.2)\n",
      "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /anaconda3/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spaCy) (4.32.1)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spaCy) (0.5.1)\n",
      "Installing collected packages: srsly, murmurhash, cymem, preshed, plac, blis, wasabi, importlib-metadata, catalogue, thinc, spaCy\n",
      "  Found existing installation: importlib-metadata 0.17\n",
      "    Uninstalling importlib-metadata-0.17:\n",
      "      Successfully uninstalled importlib-metadata-0.17\n",
      "Successfully installed blis-0.4.1 catalogue-0.0.8 cymem-2.0.3 importlib-metadata-0.23 murmurhash-1.0.2 plac-1.1.3 preshed-3.0.2 spaCy-2.2.3 srsly-0.2.0 thinc-7.3.1 wasabi-0.4.0\n",
      "Collecting en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0MB 411kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /anaconda3/lib/python3.7/site-packages (from en_core_web_sm==2.2.5) (2.2.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.3.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.22.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.2.0)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (41.0.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.16.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /anaconda3/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.32.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (0.23)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.6.16)\n",
      "Requirement already satisfied: zipp>=0.5 in /anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (0.5.1)\n",
      "Building wheels for collected packages: en-core-web-sm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /private/var/folders/8s/0g12lz7s0v770dh2kqrb1pt40000gp/T/pip-ephem-wheel-cache-sxcz2erd/wheels/6a/47/fb/6b5a0b8906d8e8779246c67d4658fd8a544d4a03a75520197a\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.2.5\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "//anaconda3/lib/python3.7/site-packages/en_core_web_sm -->\n",
      "//anaconda3/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spaCy\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next few cells to load libaries and pre-defined functions which will help us later to complete various challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/callysto/hackathon/master/Group1_Book/helper_code/book1.py -P helper_code -nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries and helper code\n",
    "import pandas as pd\n",
    "\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "\n",
    "colors20 = ['#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', \n",
    "          '#f032e6', '#bcf60c', '#fabebe', '#008080', '#e6beff', '#9a6324', '#fffac8', \n",
    "          '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000075', '#808080', '#ffffff', '#000000']\n",
    "\n",
    "# to enable plotting in colab\n",
    "def enable_plotly_in_cell():\n",
    "    import IPython\n",
    "    from plotly.offline import init_notebook_mode\n",
    "    display(IPython.core.display.HTML('''<script src=\"/static/components/requirejs/require.js\"></script>'''))\n",
    "    init_notebook_mode(connected=False)\n",
    "get_ipython().events.register('pre_run_cell', enable_plotly_in_cell) \n",
    "\n",
    "from helper_code.book1 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Download  book from project Guttenberg website\n",
    "\n",
    "**[Project Gutenberg](https://www.gutenberg.org/)** is a digital library with more than 60,000 free eBooks. You can see most popular books downloaded from Guttenberg website [here](http://www.gutenberg.org/ebooks/search/?sort_order=downloads). Can you see *Alice's Adventures in Wonderland* in that list?\n",
    "\n",
    "We've copied the book into cloud storage, and can import it into this notebook. Executing cells below will also make you aware of some interesting statistics about the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file name for the book\n",
    "alice_filename = \"alice.txt\"\n",
    "\n",
    "# copying book from cloud object storage\n",
    "alice_url=\"https://swift-yeg.cloud.cybera.ca:8080/v1/AUTH_d22d1e3f28be45209ba8f660295c84cf/hackaton/alice.txt\"\n",
    "# or directly from Project Gutenberg\n",
    "#alice_url=\"http://www.gutenberg.org/cache/epub/19033/pg19033.txt\"\n",
    "urllib.request.urlretrieve(alice_url, alice_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the book into variable 'book'\n",
    "with open(alice_filename, 'r') as text_file:\n",
    "    book = text_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the entire book on the screen\n",
    "print(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many characters (letters, numbers, spaces, etc.) are there in the book?\n",
    "len(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the book by chapter\n",
    "chapters = re.split(\"CHAPTER\\s+[IVXLCDM]+.\", book)\n",
    "\n",
    "# strip off any whitespace at the very beginning and very end of each chapter.\n",
    "chapters = [chapter.strip() for chapter in chapters]\n",
    "\n",
    "# remove tabs\n",
    "chapters = [re.sub(\"\\n\", \" \", c) for c in chapters]\n",
    "\n",
    "# select only chapters that have more than 1000 characters (to exclude table of contents, title, etc.)\n",
    "chapters = [c for c in chapters if len(c)>1000]\n",
    " \n",
    "# number of chapters\n",
    "print(len(chapters), \" chapters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a dataframe by selecting only nouns, proper nouns, verbs, and adjectives per chapter\n",
    "\n",
    "We just printed the entire book on the screen, however it is in an unstructured format. It will be easier to analyze the content if it is in a tabulated format.\n",
    "\n",
    "Run the following cells to create a dataframe which tells about various characteristics of a word in the book. Description for each of the columns of the dataframe is provided below:\n",
    "\n",
    "- **text**: actual word\n",
    "- **part-of-speech**:  ADJ, PROPN, VERB, or NOUN\n",
    "- **lemma**: headword\n",
    "- **chapter**: chapter number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running this cell will take 3-5 mins!!!\n",
    "\n",
    "#create a dataframe from the book\n",
    "book_df = get_book_df(chapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show first 5 rows of the dataframe\n",
    "book_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluding lemma equal to '’s' and '’'\n",
    "book_df = book_df[(book_df[\"lemma\"]!='’s') & (book_df[\"lemma\"]!='’')]\n",
    "\n",
    "# how many rows (individual words) and columns do we have?\n",
    "book_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything is set up for text crunching. Your group can go through the *Alice's Adventures in Wonderland* analysis below and work on challenges. \n",
    "\n",
    "**While working on the challenges, feel free to add new code/markdown cells as needed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Total number of adjectives, nouns, proper nouns, and verbs in the book\n",
    "\n",
    "Let's count the number of adjectives, nouns, proper nouns, and verbs (also known as *part-of-speech tags*) in the book. Would it be possible to do this manually?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by \"part-of-speech\" column and count the number of rows\n",
    "counts_by_part_of_speech = book_df.groupby(\"part-of-speech\").size()\n",
    "\n",
    "# create additional column - count\n",
    "counts_by_part_of_speech = counts_by_part_of_speech.reset_index(name=\"count\")\n",
    "\n",
    "counts_by_part_of_speech "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pie chart\n",
    "counts_by_part_of_speech.iplot(kind=\"pie\",values=\"count\",labels=\"part-of-speech\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: \n",
    " - If you change `groupby(\"part-of-speech\")` to `groupby(\"chapter\")`, what will it give you?\n",
    " - Can you create a pie chart showing percentage of all part-of-speech tags in each chapter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Number of adjectives/nouns/proper nouns and verbs per chapter\n",
    "\n",
    "Let's count each of the part-of-speech tags individually in each of the chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call function to get total number of all parts of speech per chapter -  its defined in the top part\n",
    "speech_parts_by_chapter = get_speechparts_by_chapter(book_df)\n",
    "speech_parts_by_chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new kind of plot - area\n",
    "speech_parts_by_chapter.iplot(kind=\"area\", fill=True, xTitle=\"Chapter\", yTitle=\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges:\n",
    " - Experiment with plots: Modify `iplot(kind=\"area\",fill=True)` to `iplot()`, `iplot(kind=\"bar\")` or  `iplot(kind = \"bar\",barmode=\"stack\")`.\n",
    " - Which type of plot can better visualize the chapter with the largest number of verbs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternate way to find the chapter with the maximum number of words is **sorting**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort_values() function - sorts by a column or set of columns\n",
    "speech_parts_by_chapter.sort_values(\"VERB\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges:\n",
    "\n",
    " - Find the  chapter that has the most **NOUN**s\n",
    " - Find the chapter that has the **fewest** adjectives\n",
    " - Plot the grouped bar chart to visualize nouns and adjectives for each chapter\n",
    " - Try two new kinds of plots - [boxplots](https://www.mathsisfun.com/definitions/box-and-whisker-plot.html) and [histograms](https://www.mathsisfun.com/data/histograms.html). Can you figure out how to interpret them?\n",
    "     - use `iplot(kind=\"box\")`\n",
    "     - use `iplot(kind=\"histogram\",subplots=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Part C: Top ten most common words\n",
    "\n",
    "Let's find top ten most used words in the book. Is this possible without computers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# call function to count the number of rows  for every \"lemma\" - its defined in the top portion of the notebook\n",
    "word_counts = get_counts(book_df, \"lemma\")\n",
    "\n",
    "# print top 10 most frequent words on the screen\n",
    "word_counts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Challenges:\n",
    " - Use \"text\" column instead of \"lemma\". Do you get different results? Why?\n",
    " - Visualize the results using the plot style of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D:  Top ten most common adjectives \n",
    "\n",
    "Let's extract top ten most used adjectives in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset only to adjectives\n",
    "adjectives = book_df[book_df[\"part-of-speech\"]==\"ADJ\"]\n",
    "\n",
    "adjectives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call function to count the number of adjectives\n",
    "adjective_counts = get_counts(adjectives, \"lemma\")\n",
    "\n",
    "adjective_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the top 10 adjectives\n",
    "adjective_counts.head(10).iplot(kind=\"bar\",xTitle=\"Lemma\",yTitle=\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges\n",
    " - Similar to words and adjectives, can you find the top 10 most common nouns and verbs?\n",
    " - Plot the results using the chart type of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part E: Proper nouns varying from chapter to chapter\n",
    "\n",
    "Now that we know how to find top few words in the book, let's analyze how the top 15 proper nouns vary by chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset with only proper nouns\n",
    "propnouns = book_df[book_df[\"part-of-speech\"]==\"PROPN\"]\n",
    "propnouns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of the most frequent proper nouns do we want to analyse\n",
    "num_words = 15\n",
    "\n",
    "# call function to count the number of proper nouns \n",
    "top_propnouns = get_counts(propnouns, \"lemma\")\n",
    "\n",
    "# get the row names(index) for top proper nouns \n",
    "top_propnouns = top_propnouns.head(num_words).index\n",
    "\n",
    "# transform them into list\n",
    "top_propnouns = list(top_propnouns)\n",
    "\n",
    "# print on the screen\n",
    "top_propnouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset with only the top proper nouns\n",
    "character_by_chapter = book_df[book_df[\"lemma\"].isin(top_propnouns)]\n",
    "character_by_chapter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# what is the distribution of top proper nouns per chapter?\n",
    "# call function to form resulting dataframe - it's defined in the top portion of the notebook\n",
    "counts_by_chapter = get_counts_by_chapters(character_by_chapter)\n",
    "\n",
    "# display on the screen\n",
    "counts_by_chapter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# who are the main characters in each chapter?\n",
    "# using colors20 from the top of the notebook to extend the default number of colors\n",
    "counts_by_chapter.iplot(kind=\"bar\",barmode = \"stack\", xTitle=\"Chapter\",yTitle=\"Counts\",colors=colors20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges:\n",
    " - Change the number of proper nouns (change `num_words`) to any other positive number and visualize how the bar chart changes.\n",
    " - Repeat the exercise (i.e. Part E) for adjectives, nouns, and verbs. Can you guess the story line for one of the chapters based on these plots?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part F: Explore the \"Adventures of Tom Sawyer\" book (optional)\n",
    "\n",
    "\n",
    "From Project Gutenberg, \"The Adventures of Tom Sawyer\" book is also available and stored in the cloud storage. You can repeat the hackathon challenges with this book and create visualizations.\n",
    "\n",
    "**Note that this section is not mandatory.**\n",
    "\n",
    "Run the following code cell to download the book from cloud and bring it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file name for the book\n",
    "tom_filename = \"tom.txt\"\n",
    "\n",
    "# copying book from cloud object storage\n",
    "tom_url=\"https://swift-yeg.cloud.cybera.ca:8080/v1/AUTH_d22d1e3f28be45209ba8f660295c84cf/hackaton/tom.txt\"\n",
    "urllib.request.urlretrieve(tom_url, tom_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This workbook analyzes the **Alice's Adventures in Wonderland** book with the help of python code blocks. The book is obtained from Project Gutenberg and part-of-speech tags are counted for the book as well as chapters. Also, commonly used words are identified and various relevant challenges are addressed. \n",
    "\n",
    "By taking part in this hackathon and completing these challenges, you learnt how to analyze big dataset which is impractical to do manually, create visualizations and most importantly, developed [*computational thinking*](https://en.wikipedia.org/wiki/Computational_thinking) abilities which can be used to solve various problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://github.com/callysto/callysto-sample-notebooks/blob/master/notebooks/images/Callysto_Notebook-Banners_Bottom_06.06.18.jpg?raw=true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
